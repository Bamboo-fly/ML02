{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d016fec8",
   "metadata": {},
   "source": [
    "## 实验二  神经网络\n",
    "### 第1.1题  标准BP算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f879dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Wine,数据预处理  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#读数据\n",
    "wine = np.genfromtxt(\"wine_data-2.csv\", delimiter=\",\", skip_header=1)  #  二分类任务\n",
    "X = wine[:, 0:13]\n",
    "y = wine[:, 13]\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_st = sc.fit_transform(X)  # 对样本的各属性值进行准化\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_st, y)  # 默认取出97个样本作为测试集，33个作为测试集\n",
    "# print(x_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4937efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 0.003  #学习率\n",
    "# 以13个特征值作为输入，1个神经元作为输出（输出>=0.5为1类，<0.5为0类），中间隐藏层50个神经元\n",
    "v = np.random.random((13, 100)) * 2 - 1\n",
    "w = np.random.random((50, 3)) * 2 - 1\n",
    "\n",
    "label_train = LabelBinarizer().fit_transform(y_train)\n",
    "# print(label_train)\n",
    "label_test = LabelBinarizer().fit_transform(y_test)\n",
    "# print(label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0cbba844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义激活函数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#激活函数的导数\n",
    "def d_sigmoid(x):\n",
    "    return x * (1 - x)  #%% md\n",
    "\n",
    "## 实验二  神经网络\n",
    "### 第1.1题  标准BP算法"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#  Wine,数据预处理\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#读数据\n",
    "wine = np.genfromtxt(\"wine_data-2.csv\", delimiter=\",\", skip_header=1)  #  二分类任务\n",
    "X = wine[:, 0:13]\n",
    "y = wine[:, 13]\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_st = sc.fit_transform(X)  # 对样本的各属性值进行准化\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_st, y)  # 默认取出97个样本作为测试集，33个作为测试集\n",
    "# print(x_train.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rate = 0.003  #学习率\n",
    "# 以13个特征值作为输入，1个神经元作为输出（输出>=0.5为1类，<0.5为0类），中间隐藏层50个神经元\n",
    "v = np.random.random((13, 100)) * 2 - 1\n",
    "w = np.random.random((50, 3)) * 2 - 1\n",
    "\n",
    "label_train = LabelBinarizer().fit_transform(y_train)\n",
    "# print(label_train)\n",
    "label_test = LabelBinarizer().fit_transform(y_test)\n",
    "# print(label_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 定义激活函数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#激活函数的导数\n",
    "def d_sigmoid(x):\n",
    "    return x * (1 - x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 训练\n",
    "def train(x, y, outputs_dim=3, eta=0.05, max_iter=1000):\n",
    "    #   outputs_dim  输出层神经元个数\n",
    "    hiden_dim = 50  # 隐层神经元个数\n",
    "\n",
    "    #定义权重\n",
    "    w1 = np.zeros((x.shape[1], hiden_dim))  #  （13，50）<--- 矩阵维数\n",
    "    b1 = np.zeros((1, hiden_dim))  #  （1，50）\n",
    "    w2 = np.zeros((hiden_dim, 1))  #  （50，1）\n",
    "    b2 = np.zeros((outputs_dim, 1))  #  1 X 1\n",
    "\n",
    "    losslist = []  #损失列表\n",
    "\n",
    "    for ite in range(max_iter):\n",
    "        loss_per_ite = []\n",
    "        for m in range(x.shape[0]):  # 遍历样本\n",
    "            xi, yi = x[m, :], y[m, :]\n",
    "\n",
    "            xi, yi = xi.reshape(1, xi.shape[0]), yi.reshape(1, yi.shape[0])\n",
    "\n",
    "            ##前向传播\n",
    "            u1 = np.dot(xi, w1) + b1\n",
    "            out1 = sigmoid(u1)  # 隐含层的输出  1 X 50\n",
    "\n",
    "            u2 = np.dot(out1, w2) + b2  # (1,50) X (50，1) =（1,1）\n",
    "            out2 = sigmoid(u2)  #输出(激活)层的输出,（1,1）\n",
    "\n",
    "            loss = np.square(yi - out2) / 2\n",
    "            loss_per_ite.append(loss)\n",
    "            #             print(\"iter:\",ite,\" loss:\",loss)\n",
    "\n",
    "            ##反向传播\n",
    "            ##标准BP\n",
    "            d_out2 = -(yi - out2)  # （1,1）\n",
    "            d_u2 = d_out2 * d_sigmoid(out2)  #  gj ,（1,1） zhouzhihua   jqxx(机器学习)  P103\n",
    "\n",
    "            d_w2 = np.dot(np.transpose(out1), d_u2)  # delta(whj)，(50，1), np.transpose()--矩阵转置\n",
    "            d_b2 = d_u2  # delta(thetaj),(1, 1)\n",
    "\n",
    "            d_out1 = d_u2 * w2  #  3E/3bh   zhouzhihua   jqxx(机器学习)  P103  (1,1) 点乘 (50，1) ---> (50,1)\n",
    "\n",
    "            d_u1 = np.transpose(d_out1) * d_sigmoid(out1)  #  -eh    d_out1: (50, 1) , out1: 1 X 50 ，改成矩阵点乘\n",
    "            #  shapes (13,1) and (1,50) --->  (13 , 50)\n",
    "            d_w1 = np.dot(np.transpose(xi), d_u1)  # delta(vih)\n",
    "            d_b1 = d_u1  # delta(rh)  (1 , 50)\n",
    "            ##更新权重\n",
    "            w1 = w1 - eta * d_w1\n",
    "            w2 = w2 - eta * d_w2\n",
    "            b1 = b1 - eta * d_b1\n",
    "            b2 = b2 - eta * d_b2\n",
    "        losslist.append(np.mean(loss_per_ite))\n",
    "\n",
    "    ##Loss可视化，损失函数曲线\n",
    "    plt.figure()\n",
    "    plt.plot([i + 1 for i in range(max_iter)], losslist)\n",
    "    plt.legend(['standard BP'])\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()\n",
    "    return w1, w2, b1, b2\n",
    "\n",
    "\n",
    "######### 训练神经网络 #########\n",
    "w1, w2, b1, b2 = train(x_train, label_train, 1)  # 成功训练\n",
    "test_label_list = []  #损失列表\n",
    "for m in range(x_test.shape[0]):\n",
    "    xi, yi = x_test[m, :], label_test[m, :]\n",
    "    xi, yi = xi.reshape(1, xi.shape[0]), yi.reshape(1, yi.shape[0])\n",
    "    ##前向传播\n",
    "    u1 = np.dot(xi, w1) + b1\n",
    "    out1 = sigmoid(u1)  # 隐含层的输出  1 X 50\n",
    "    u2 = np.dot(out1, w2) + b2\n",
    "    out2 = sigmoid(u2)  #激活层\n",
    "    if out2 >= 0.5:\n",
    "        test_label_list.append(1)\n",
    "    else:\n",
    "        test_label_list.append(0)\n",
    "\n",
    "re = 0  # 记录测试正确的样本数\n",
    "\n",
    "# 计算测试精度\n",
    "for i in range(len(y_test)):\n",
    "    if test_label_list[i] == y_test[i]:\n",
    "        re = re + 1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# 计算、输出测试精度\n",
    "acc = re / len(y_test)\n",
    "print(\"测试精度acc =\", acc)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#进行训练\n",
    "# w1,w2,b1,b2 = train(x_train,label_train,1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 训练\n",
    "def train(x, y, outputs_dim=3, eta=0.05, max_iter=1000):\n",
    "    #   outputs_dim  输出层神经元个数\n",
    "    hiden_dim = 50  # 隐层神经元个数\n",
    "\n",
    "    #定义权重\n",
    "    w1 = np.zeros((x.shape[1], hiden_dim))  #  （13，50）<--- 矩阵维数\n",
    "    b1 = np.zeros((1, hiden_dim))  #  （1，50）\n",
    "    w2 = np.zeros((hiden_dim, 1))  #  （50，1）\n",
    "    b2 = np.zeros((outputs_dim, 1))  #  1 X 1\n",
    "\n",
    "    losslist = []  #损失列表\n",
    "\n",
    "    for ite in range(max_iter):\n",
    "        loss_per_ite = []\n",
    "        for m in range(x.shape[0]):  # 遍历样本\n",
    "            xi, yi = x[m, :], y[m, :]\n",
    "\n",
    "            xi, yi = xi.reshape(1, xi.shape[0]), yi.reshape(1, yi.shape[0])\n",
    "\n",
    "            ##前向传播   \n",
    "            u1 = np.dot(xi, w1) + b1\n",
    "            out1 = sigmoid(u1)  # 隐含层的输出  1 X 50\n",
    "\n",
    "            u2 = np.dot(out1, w2) + b2  # (1,50) X (50，1) =（1,1）\n",
    "            out2 = sigmoid(u2)  #输出(激活)层的输出,（1,1）\n",
    "\n",
    "            loss = np.square(yi - out2) / 2\n",
    "            loss_per_ite.append(loss)\n",
    "            #             print(\"iter:\",ite,\" loss:\",loss)\n",
    "\n",
    "            ##反向传播\n",
    "            ##标准BP\n",
    "            d_out2 = -(yi - out2)  # （1,1）\n",
    "            d_u2 = d_out2 * d_sigmoid(out2)  #  gj ,（1,1） zhouzhihua   jqxx(机器学习)  P103\n",
    "\n",
    "            d_w2 = np.dot(np.transpose(out1), d_u2)  # delta(whj)，(50，1), np.transpose()--矩阵转置\n",
    "            d_b2 = d_u2  # delta(thetaj),(1, 1)\n",
    "\n",
    "            d_out1 = d_u2 * w2  #  3E/3bh   zhouzhihua   jqxx(机器学习)  P103  (1,1) 点乘 (50，1) ---> (50,1)\n",
    "\n",
    "            d_u1 = np.transpose(d_out1) * d_sigmoid(out1)  #  -eh    d_out1: (50, 1) , out1: 1 X 50 ，改成矩阵点乘\n",
    "            #  shapes (13,1) and (1,50) --->  (13 , 50)\n",
    "            d_w1 = np.dot(np.transpose(xi), d_u1)  # delta(vih)\n",
    "            d_b1 = d_u1  # delta(rh)  (1 , 50) \n",
    "            ##更新权重\n",
    "            w1 = w1 - eta * d_w1\n",
    "            w2 = w2 - eta * d_w2\n",
    "            b1 = b1 - eta * d_b1\n",
    "            b2 = b2 - eta * d_b2\n",
    "        losslist.append(np.mean(loss_per_ite))\n",
    "\n",
    "    ##Loss可视化，损失函数曲线\n",
    "    plt.figure()\n",
    "    plt.plot([i + 1 for i in range(max_iter)], losslist)\n",
    "    plt.legend(['standard BP'])\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()\n",
    "    return w1, w2, b1, b2\n",
    "\n",
    "\n",
    "######### 训练神经网络 #########\n",
    "w1, w2, b1, b2 = train(x_train, label_train, 1)  # 成功训练\n",
    "test_label_list = []  #损失列表\n",
    "for m in range(x_test.shape[0]):\n",
    "    xi, yi = x_test[m, :], label_test[m, :]\n",
    "    xi, yi = xi.reshape(1, xi.shape[0]), yi.reshape(1, yi.shape[0])\n",
    "    ##前向传播   \n",
    "    u1 = np.dot(xi, w1) + b1\n",
    "    out1 = sigmoid(u1)  # 隐含层的输出  1 X 50\n",
    "    u2 = np.dot(out1, w2) + b2\n",
    "    out2 = sigmoid(u2)  #激活层\n",
    "    if out2 >= 0.5:\n",
    "        test_label_list.append(1)\n",
    "    else:\n",
    "        test_label_list.append(0)\n",
    "\n",
    "re = 0  # 记录测试正确的样本数\n",
    "\n",
    "# 计算测试精度\n",
    "for i in range(len(y_test)):\n",
    "    if test_label_list[i] == y_test[i]:\n",
    "        re = re + 1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# 计算、输出测试精度\n",
    "acc = re / len(y_test)\n",
    "print(\"测试精度acc =\", acc)#%% md\n",
    "## 实验二  神经网络\n",
    "### 第1.1题  标准BP算法"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#  Wine,数据预处理\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#读数据\n",
    "wine = np.genfromtxt(\"wine_data-2.csv\", delimiter=\",\", skip_header=1)  #  二分类任务\n",
    "X = wine[:, 0:13]\n",
    "y = wine[:, 13]\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_st = sc.fit_transform(X)  # 对样本的各属性值进行准化\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_st, y)  # 默认取出97个样本作为测试集，33个作为测试集\n",
    "# print(x_train.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rate = 0.003  #学习率\n",
    "# 以13个特征值作为输入，1个神经元作为输出（输出>=0.5为1类，<0.5为0类），中间隐藏层50个神经元\n",
    "v = np.random.random((13, 100)) * 2 - 1\n",
    "w = np.random.random((50, 3)) * 2 - 1\n",
    "\n",
    "label_train = LabelBinarizer().fit_transform(y_train)\n",
    "# print(label_train)\n",
    "label_test = LabelBinarizer().fit_transform(y_test)\n",
    "# print(label_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 定义激活函数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#激活函数的导数\n",
    "def d_sigmoid(x):\n",
    "    return x * (1 - x)  #%% md\n",
    "\n",
    "## 实验二  神经网络\n",
    "### 第1.1题  标准BP算法"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#  Wine,数据预处理\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#读数据\n",
    "wine = np.genfromtxt(\"wine_data-2.csv\", delimiter=\",\", skip_header=1)  #  二分类任务\n",
    "X = wine[:, 0:13]\n",
    "y = wine[:, 13]\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_st = sc.fit_transform(X)  # 对样本的各属性值进行准化\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_st, y)  # 默认取出97个样本作为测试集，33个作为测试集\n",
    "# print(x_train.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rate = 0.003  #学习率\n",
    "# 以13个特征值作为输入，1个神经元作为输出（输出>=0.5为1类，<0.5为0类），中间隐藏层50个神经元\n",
    "v = np.random.random((13, 100)) * 2 - 1\n",
    "w = np.random.random((50, 3)) * 2 - 1\n",
    "\n",
    "label_train = LabelBinarizer().fit_transform(y_train)\n",
    "# print(label_train)\n",
    "label_test = LabelBinarizer().fit_transform(y_test)\n",
    "# print(label_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 定义激活函数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#激活函数的导数\n",
    "def d_sigmoid(x):\n",
    "    return x * (1 - x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 训练\n",
    "def train(x, y, outputs_dim=3, eta=0.05, max_iter=1000):\n",
    "    #   outputs_dim  输出层神经元个数\n",
    "    hiden_dim = 50  # 隐层神经元个数\n",
    "\n",
    "    #定义权重\n",
    "    w1 = np.zeros((x.shape[1], hiden_dim))  #  （13，50）<--- 矩阵维数\n",
    "    b1 = np.zeros((1, hiden_dim))  #  （1，50）\n",
    "    w2 = np.zeros((hiden_dim, 1))  #  （50，1）\n",
    "    b2 = np.zeros((outputs_dim, 1))  #  1 X 1\n",
    "\n",
    "    losslist = []  #损失列表\n",
    "\n",
    "    for ite in range(max_iter):\n",
    "        loss_per_ite = []\n",
    "        for m in range(x.shape[0]):  # 遍历样本\n",
    "            xi, yi = x[m, :], y[m, :]\n",
    "\n",
    "            xi, yi = xi.reshape(1, xi.shape[0]), yi.reshape(1, yi.shape[0])\n",
    "\n",
    "            ##前向传播\n",
    "            u1 = np.dot(xi, w1) + b1\n",
    "            out1 = sigmoid(u1)  # 隐含层的输出  1 X 50\n",
    "\n",
    "            u2 = np.dot(out1, w2) + b2  # (1,50) X (50，1) =（1,1）\n",
    "            out2 = sigmoid(u2)  #输出(激活)层的输出,（1,1）\n",
    "\n",
    "            loss = np.square(yi - out2) / 2\n",
    "            loss_per_ite.append(loss)\n",
    "            #             print(\"iter:\",ite,\" loss:\",loss)\n",
    "\n",
    "            ##反向传播\n",
    "            ##标准BP\n",
    "            d_out2 = -(yi - out2)  # （1,1）\n",
    "            d_u2 = d_out2 * d_sigmoid(out2)  #  gj ,（1,1） zhouzhihua   jqxx(机器学习)  P103\n",
    "\n",
    "            d_w2 = np.dot(np.transpose(out1), d_u2)  # delta(whj)，(50，1), np.transpose()--矩阵转置\n",
    "            d_b2 = d_u2  # delta(thetaj),(1, 1)\n",
    "\n",
    "            d_out1 = d_u2 * w2  #  3E/3bh   zhouzhihua   jqxx(机器学习)  P103  (1,1) 点乘 (50，1) ---> (50,1)\n",
    "\n",
    "            d_u1 = np.transpose(d_out1) * d_sigmoid(out1)  #  -eh    d_out1: (50, 1) , out1: 1 X 50 ，改成矩阵点乘\n",
    "            #  shapes (13,1) and (1,50) --->  (13 , 50)\n",
    "            d_w1 = np.dot(np.transpose(xi), d_u1)  # delta(vih)\n",
    "            d_b1 = d_u1  # delta(rh)  (1 , 50)\n",
    "            ##更新权重\n",
    "            w1 = w1 - eta * d_w1\n",
    "            w2 = w2 - eta * d_w2\n",
    "            b1 = b1 - eta * d_b1\n",
    "            b2 = b2 - eta * d_b2\n",
    "        losslist.append(np.mean(loss_per_ite))\n",
    "\n",
    "    ##Loss可视化，损失函数曲线\n",
    "    plt.figure()\n",
    "    plt.plot([i + 1 for i in range(max_iter)], losslist)\n",
    "    plt.legend(['standard BP'])\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()\n",
    "    return w1, w2, b1, b2\n",
    "\n",
    "\n",
    "######### 训练神经网络 #########\n",
    "w1, w2, b1, b2 = train(x_train, label_train, 1)  # 成功训练\n",
    "test_label_list = []  #损失列表\n",
    "for m in range(x_test.shape[0]):\n",
    "    xi, yi = x_test[m, :], label_test[m, :]\n",
    "    xi, yi = xi.reshape(1, xi.shape[0]), yi.reshape(1, yi.shape[0])\n",
    "    ##前向传播\n",
    "    u1 = np.dot(xi, w1) + b1\n",
    "    out1 = sigmoid(u1)  # 隐含层的输出  1 X 50\n",
    "    u2 = np.dot(out1, w2) + b2\n",
    "    out2 = sigmoid(u2)  #激活层\n",
    "    if out2 >= 0.5:\n",
    "        test_label_list.append(1)\n",
    "    else:\n",
    "        test_label_list.append(0)\n",
    "\n",
    "re = 0  # 记录测试正确的样本数\n",
    "\n",
    "# 计算测试精度\n",
    "for i in range(len(y_test)):\n",
    "    if test_label_list[i] == y_test[i]:\n",
    "        re = re + 1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# 计算、输出测试精度\n",
    "acc = re / len(y_test)\n",
    "print(\"测试精度acc =\", acc)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#进行训练\n",
    "# w1,w2,b1,b2 = train(x_train,label_train,1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 训练\n",
    "def train(x, y, outputs_dim=3, eta=0.05, max_iter=1000):\n",
    "    #   outputs_dim  输出层神经元个数\n",
    "    hiden_dim = 50  # 隐层神经元个数\n",
    "\n",
    "    #定义权重\n",
    "    w1 = np.zeros((x.shape[1], hiden_dim))  #  （13，50）<--- 矩阵维数\n",
    "    b1 = np.zeros((1, hiden_dim))  #  （1，50）\n",
    "    w2 = np.zeros((hiden_dim, 1))  #  （50，1）\n",
    "    b2 = np.zeros((outputs_dim, 1))  #  1 X 1\n",
    "\n",
    "    losslist = []  #损失列表\n",
    "\n",
    "    for ite in range(max_iter):\n",
    "        loss_per_ite = []\n",
    "        for m in range(x.shape[0]):  # 遍历样本\n",
    "            xi, yi = x[m, :], y[m, :]\n",
    "\n",
    "            xi, yi = xi.reshape(1, xi.shape[0]), yi.reshape(1, yi.shape[0])\n",
    "\n",
    "            ##前向传播\n",
    "            u1 = np.dot(xi, w1) + b1\n",
    "            out1 = sigmoid(u1)  # 隐含层的输出  1 X 50\n",
    "\n",
    "            u2 = np.dot(out1, w2) + b2  # (1,50) X (50，1) =（1,1）\n",
    "            out2 = sigmoid(u2)  #输出(激活)层的输出,（1,1）\n",
    "\n",
    "            loss = np.square(yi - out2) / 2\n",
    "            loss_per_ite.append(loss)\n",
    "            #             print(\"iter:\",ite,\" loss:\",loss)\n",
    "\n",
    "            ##反向传播\n",
    "            ##标准BP\n",
    "            d_out2 = -(yi - out2)  # （1,1）\n",
    "            d_u2 = d_out2 * d_sigmoid(out2)  #  gj ,（1,1） zhouzhihua   jqxx(机器学习)  P103\n",
    "\n",
    "            d_w2 = np.dot(np.transpose(out1), d_u2)  # delta(whj)，(50，1), np.transpose()--矩阵转置\n",
    "            d_b2 = d_u2  # delta(thetaj),(1, 1)\n",
    "\n",
    "            d_out1 = d_u2 * w2  #  3E/3bh   zhouzhihua   jqxx(机器学习)  P103  (1,1) 点乘 (50，1) ---> (50,1)\n",
    "\n",
    "            d_u1 = np.transpose(d_out1) * d_sigmoid(out1)  #  -eh    d_out1: (50, 1) , out1: 1 X 50 ，改成矩阵点乘\n",
    "            #  shapes (13,1) and (1,50) --->  (13 , 50)\n",
    "            d_w1 = np.dot(np.transpose(xi), d_u1)  # delta(vih)\n",
    "            d_b1 = d_u1  # delta(rh)  (1 , 50)\n",
    "            ##更新权重\n",
    "            w1 = w1 - eta * d_w1\n",
    "            w2 = w2 - eta * d_w2\n",
    "            b1 = b1 - eta * d_b1\n",
    "            b2 = b2 - eta * d_b2\n",
    "        losslist.append(np.mean(loss_per_ite))\n",
    "\n",
    "    ##Loss可视化，损失函数曲线\n",
    "    plt.figure()\n",
    "    plt.plot([i + 1 for i in range(max_iter)], losslist)\n",
    "    plt.legend(['standard BP'])\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()\n",
    "    return w1, w2, b1, b2\n",
    "\n",
    "\n",
    "######### 训练神经网络 #########\n",
    "w1, w2, b1, b2 = train(x_train, label_train, 1)  # 成功训练\n",
    "test_label_list = []  #损失列表\n",
    "for m in range(x_test.shape[0]):\n",
    "    xi, yi = x_test[m, :], label_test[m, :]\n",
    "    xi, yi = xi.reshape(1, xi.shape[0]), yi.reshape(1, yi.shape[0])\n",
    "    ##前向传播\n",
    "    u1 = np.dot(xi, w1) + b1\n",
    "    out1 = sigmoid(u1)  # 隐含层的输出  1 X 50\n",
    "    u2 = np.dot(out1, w2) + b2\n",
    "    out2 = sigmoid(u2)  #激活层\n",
    "    if out2 >= 0.5:\n",
    "        test_label_list.append(1)\n",
    "    else:\n",
    "        test_label_list.append(0)\n",
    "\n",
    "re = 0  # 记录测试正确的样本数\n",
    "\n",
    "# 计算测试精度\n",
    "for i in range(len(y_test)):\n",
    "    if test_label_list[i] == y_test[i]:\n",
    "        re = re + 1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# 计算、输出测试精度\n",
    "acc = re / len(y_test)\n",
    "print(\"测试精度acc =\", acc)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#进行训练\n",
    "# w1,w2,b1,b2 = train(x_train,label_train,1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "144977df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#进行训练\n",
    "# w1,w2,b1,b2 = train(x_train,label_train,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d6073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
